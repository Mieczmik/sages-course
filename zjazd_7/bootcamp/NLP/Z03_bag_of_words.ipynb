{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wektorowa reprezentacja tekstu\n",
    "\n",
    "W najprostszym przypadku budowanie reprezentacji wektorowej polega na zliczaniu ilości słów (ciągów). Każde słowo to atrybut a wartość w danym atrybucie to ilość wystąpniń tego słowa. \n",
    "\n",
    "\n",
    "## bag-of-words\n",
    "\n",
    "Najprostsza reprezentacja to reprezentacja bag-of-words: https://en.wikipedia.org/wiki/Bag-of-words_model. \n",
    "\n",
    "Kazdy wektor zapisany jest w rzadkiej reprezentacji jako że słownik może być duży a dokument mały."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "* Weźmy tekst i podzielmy na tokeny. \n",
    "\n",
    "* Proszę stworzyć słownik słów oraz policzyć częstości każdego słowa. \n",
    "\n",
    "* Można użyć Counter - daje nam słownik: https://docs.python.org/2/library/collections.html#collections.Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\n"
     ]
    }
   ],
   "source": [
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----słownik----\n",
      "{'corpus', 'to', 'fourgrams', 'bigrams', 'unigrams', 'fivegrams', 'NLTK', ',', 'a', 'files', 'I', 'of', 'breaks', 'trigrams', 'txt', 'that', 'program', 'write', 'need', '.', 'and', 'large', ')', '(', 'collection', 'into', 'in'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "unigrams = set(tokens)\n",
    "print(\"-----słownik----\")\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----licznik----\n",
      "Counter({'a': 5, ',': 3, 'I': 2, 'need': 2, 'to': 2, 'write': 2, 'program': 2, 'in': 2, 'NLTK': 2, 'that': 2, 'breaks': 2, 'corpus': 2, '(': 1, 'large': 1, 'collection': 1, 'of': 1, 'txt': 1, 'files': 1, ')': 1, 'into': 1, 'unigrams': 1, 'bigrams': 1, 'trigrams': 1, 'fourgrams': 1, 'and': 1, 'fivegrams': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"-----licznik----\")\n",
    "counts1 = Counter(tokens)\n",
    "print(counts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----klucze---\n",
      "dict_keys(['I', 'need', 'to', 'write', 'a', 'program', 'in', 'NLTK', 'that', 'breaks', 'corpus', '(', 'large', 'collection', 'of', 'txt', 'files', ')', 'into', 'unigrams', ',', 'bigrams', 'trigrams', 'fourgrams', 'and', 'fivegrams', '.'])\n",
      "----wartości--\n",
      "dict_values([2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"-----klucze---\")\n",
    "print(counts1.keys())\n",
    "print(\"----wartości--\")\n",
    "print(counts1.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--miejsce--\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"--miejsce--\")\n",
    "print(counts1['I'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad (dla chętnych)\n",
    "\n",
    "Zostańmy przy unigramach. Proszę napisać reprezentację bag-of-words własnoręcznie. \n",
    "\n",
    "W tym celu:\n",
    "\n",
    " * tworzymy słownik słów na podstawie zdanego tekstu (metoda fit)\n",
    "\n",
    " * dla stokenizowanego dokumentu, tworzymy macierz częstości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'txt', ',', 'of', 'and', 'fourgrams', 'write', 'corpus', 'that', 'in', ')', 'breaks', '(', 'NLTK', 'fivegrams', 'need', 'to', 'collection', 'bigrams', 'into', 'I', 'unigrams', 'program', 'large', 'trigrams', 'a', 'files', '.'}\n",
      "--reprezentacja--\n",
      "[1. 3. 1. 1. 1. 2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 5. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(tokens):\n",
    "    ??\n",
    "\n",
    "def transform(sentence_words, words):\n",
    "    ??\n",
    "\n",
    "words = fit(tokens)\n",
    "print(words)\n",
    "print(\"--reprezentacja--\")\n",
    "representation = transform(tokens, words)\n",
    "print(representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngrams\n",
    "\n",
    "Teksto można reprezentaować też jako ciągi czyli n-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\n"
     ]
    }
   ],
   "source": [
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need'): 2, ('need', 'to'): 2, ('to', 'write'): 2, ('write', 'a'): 2, ('a', 'program'): 2, ('program', 'in'): 2, ('in', 'NLTK'): 2, ('NLTK', 'that'): 2, ('that', 'breaks'): 2, ('breaks', 'a'): 2, ('a', 'corpus'): 2, ('corpus', '('): 1, ('(', 'a'): 1, ('a', 'large'): 1, ('large', 'collection'): 1, ('collection', 'of'): 1, ('of', 'txt'): 1, ('txt', 'files'): 1, ('files', ')'): 1, (')', 'into'): 1, ('into', 'unigrams'): 1, ('unigrams', ','): 1, (',', 'bigrams'): 1, ('bigrams', ','): 1, (',', 'trigrams'): 1, ('trigrams', ','): 1, (',', 'fourgrams'): 1, ('fourgrams', 'and'): 1, ('and', 'fivegrams'): 1, ('fivegrams', '.'): 1, ('.', 'I'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts2 = Counter(bigrams)\n",
    "print(counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need', 'to'): 2, ('need', 'to', 'write'): 2, ('to', 'write', 'a'): 2, ('write', 'a', 'program'): 2, ('a', 'program', 'in'): 2, ('program', 'in', 'NLTK'): 2, ('in', 'NLTK', 'that'): 2, ('NLTK', 'that', 'breaks'): 2, ('that', 'breaks', 'a'): 2, ('breaks', 'a', 'corpus'): 2, ('a', 'corpus', '('): 1, ('corpus', '(', 'a'): 1, ('(', 'a', 'large'): 1, ('a', 'large', 'collection'): 1, ('large', 'collection', 'of'): 1, ('collection', 'of', 'txt'): 1, ('of', 'txt', 'files'): 1, ('txt', 'files', ')'): 1, ('files', ')', 'into'): 1, (')', 'into', 'unigrams'): 1, ('into', 'unigrams', ','): 1, ('unigrams', ',', 'bigrams'): 1, (',', 'bigrams', ','): 1, ('bigrams', ',', 'trigrams'): 1, (',', 'trigrams', ','): 1, ('trigrams', ',', 'fourgrams'): 1, (',', 'fourgrams', 'and'): 1, ('fourgrams', 'and', 'fivegrams'): 1, ('and', 'fivegrams', '.'): 1, ('fivegrams', '.', 'I'): 1, ('.', 'I', 'need'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts3 = Counter(trigrams)\n",
    "print(counts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need', 'to', 'write'): 2, ('need', 'to', 'write', 'a'): 2, ('to', 'write', 'a', 'program'): 2, ('write', 'a', 'program', 'in'): 2, ('a', 'program', 'in', 'NLTK'): 2, ('program', 'in', 'NLTK', 'that'): 2, ('in', 'NLTK', 'that', 'breaks'): 2, ('NLTK', 'that', 'breaks', 'a'): 2, ('that', 'breaks', 'a', 'corpus'): 2, ('breaks', 'a', 'corpus', '('): 1, ('a', 'corpus', '(', 'a'): 1, ('corpus', '(', 'a', 'large'): 1, ('(', 'a', 'large', 'collection'): 1, ('a', 'large', 'collection', 'of'): 1, ('large', 'collection', 'of', 'txt'): 1, ('collection', 'of', 'txt', 'files'): 1, ('of', 'txt', 'files', ')'): 1, ('txt', 'files', ')', 'into'): 1, ('files', ')', 'into', 'unigrams'): 1, (')', 'into', 'unigrams', ','): 1, ('into', 'unigrams', ',', 'bigrams'): 1, ('unigrams', ',', 'bigrams', ','): 1, (',', 'bigrams', ',', 'trigrams'): 1, ('bigrams', ',', 'trigrams', ','): 1, (',', 'trigrams', ',', 'fourgrams'): 1, ('trigrams', ',', 'fourgrams', 'and'): 1, (',', 'fourgrams', 'and', 'fivegrams'): 1, ('fourgrams', 'and', 'fivegrams', '.'): 1, ('and', 'fivegrams', '.', 'I'): 1, ('fivegrams', '.', 'I', 'need'): 1, ('.', 'I', 'need', 'to'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts4 = Counter(fourgrams)\n",
    "print(counts4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need', 'to', 'write', 'a'): 2, ('need', 'to', 'write', 'a', 'program'): 2, ('to', 'write', 'a', 'program', 'in'): 2, ('write', 'a', 'program', 'in', 'NLTK'): 2, ('a', 'program', 'in', 'NLTK', 'that'): 2, ('program', 'in', 'NLTK', 'that', 'breaks'): 2, ('in', 'NLTK', 'that', 'breaks', 'a'): 2, ('NLTK', 'that', 'breaks', 'a', 'corpus'): 2, ('that', 'breaks', 'a', 'corpus', '('): 1, ('breaks', 'a', 'corpus', '(', 'a'): 1, ('a', 'corpus', '(', 'a', 'large'): 1, ('corpus', '(', 'a', 'large', 'collection'): 1, ('(', 'a', 'large', 'collection', 'of'): 1, ('a', 'large', 'collection', 'of', 'txt'): 1, ('large', 'collection', 'of', 'txt', 'files'): 1, ('collection', 'of', 'txt', 'files', ')'): 1, ('of', 'txt', 'files', ')', 'into'): 1, ('txt', 'files', ')', 'into', 'unigrams'): 1, ('files', ')', 'into', 'unigrams', ','): 1, (')', 'into', 'unigrams', ',', 'bigrams'): 1, ('into', 'unigrams', ',', 'bigrams', ','): 1, ('unigrams', ',', 'bigrams', ',', 'trigrams'): 1, (',', 'bigrams', ',', 'trigrams', ','): 1, ('bigrams', ',', 'trigrams', ',', 'fourgrams'): 1, (',', 'trigrams', ',', 'fourgrams', 'and'): 1, ('trigrams', ',', 'fourgrams', 'and', 'fivegrams'): 1, (',', 'fourgrams', 'and', 'fivegrams', '.'): 1, ('fourgrams', 'and', 'fivegrams', '.', 'I'): 1, ('and', 'fivegrams', '.', 'I', 'need'): 1, ('fivegrams', '.', 'I', 'need', 'to'): 1, ('.', 'I', 'need', 'to', 'write'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts5 = Counter(fivegrams)\n",
    "print(counts5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Do budowy reprezentacji dla wielu tekstów można użyć sklearn do tej samej czynności:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\", \"I'm 20 years old.\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard. I'm 20 years old.\"\n",
    "sentences = sent_tokenize(EXAMPLE_TEXT)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'mr': 12, 'smith': 18, 'how': 10, 'are': 2, 'you': 23, 'doing': 6, 'today': 20, 'the': 19, 'weather': 21, 'is': 11, 'great': 8, 'and': 1, 'python': 15, 'awesome': 3, 'sky': 17, 'pinkish': 14, 'blue': 4, 'shouldn': 16, 'eat': 7, 'cardboard': 5, '20': 0, 'years': 22, 'old': 13}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 23)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t2\n",
      "  (1, 15)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 21)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 23)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 22)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vectorizer.transform(sentences)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 1 0 1 0 0 0 0 1 0 0 2 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad:\n",
    "\n",
    "Możemy tutaj użyć preprocessing i tokenizacje domyślną, ale można też zdefiniować własną.\n",
    "\n",
    "Proszę użyć wcześniej zdefiniowane funkcje do przetwarzania tekstu w naszym transfromerze - zobacz:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "a następnie stworzyć reprezentację **bag-of-words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n",
      "I'm 20 years old.\n",
      "{'hello': 7, 'mr.': 8, 'smith': 14, 'today': 15, 'weather': 16, 'great': 6, 'python': 12, 'awesom': 3, 'sky': 13, 'pinkish-blu': 11, \"n't\": 9, 'eat': 5, 'cardboard': 4, 'I': 2, \"'m\": 0, '20': 1, 'year': 17, 'old': 10}\n",
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n",
      "I'm 20 years old.\n",
      "[[0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#tokenizer działa na tokenach już\n",
    "def stemming_tokenizer(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokenized]\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    print(word)\n",
    "    return word\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = my_preprocessing, tokenizer=stemming_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )\n",
    "print(vectorizer.transform(sentences).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    " \n",
    "Proszę wykonać podstawową tekenizację biorąc pod uwagę:\n",
    "\n",
    " * bierzemy kolejne artykuły i dzieli go na tokeny\n",
    " * bierzemy listę tokenów i usuwamy punktory\n",
    " * bierzemy listę tokenów i usuwa liczby\n",
    " * bierzemy listę tokenów i zamieniamy na małe litery\n",
    " \n",
    "a następnie stworzyć reprezentację **bag-of-words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Vectorizer buduje nam słownik, czyli definiuje poszczególne wymiary. Mając taki słownik możemy transformować nowe zdania.\n",
    "\n",
    "Jaką reprezentację będą miały zdania:\n",
    "\n",
    "\n",
    " * \"The weather is awesome\"\n",
    " \n",
    " * \"You shouldn't eat 20 20 30 cardboards\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is awesome\n",
      "You shouldn't eat 20 20 30 cardboards\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_sentences_1 = [\"The weather is awesome\", \"You shouldn't eat 20 20 30 cardboards\"]\n",
    "\n",
    "print(vectorizer.transform(new_sentences_1).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Jaką reprezentację będą miały zdania zawierające słowo nie występujące wcześniej (\"apples\"):\n",
    "\n",
    "\n",
    " * \"You shouldn't eat apples\"\n",
    " * \"Apples apples apples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
