{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "LSI pozwala w pewnym sensie znajdowac kluczowe tematy w tekście i dla danego dokumentu określać najbliższy mu temat. Jest to podejście geometryczne. \n",
    "\n",
    "LDA jest podejściem probabilistycznym do modelowania tematów. Jest bardziej dokładny choć wolniejszy.\n",
    "\n",
    "# Model LDA - Latent Dirichlet Allocation (ukryta alokacja Dirichleta)\n",
    "\n",
    "\n",
    "Motywacja: przedstawienie tekstu jako mieszanki tematów.\n",
    "\n",
    "\n",
    "Temat - rozkład prawdopodobieństwa na zbiorze słów.\n",
    "\n",
    "\n",
    "Przykład:\n",
    "*  <s>Mam</s> gorączkę <s>i</s> katar.\n",
    "* Graliśmy <s>w</s> siatkówkę.\n",
    "* Grając <s>w</s> piłkę, wzmacniamy organizm.\n",
    "\n",
    "\n",
    "Ile \"tematów\" widzimy?\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Intuicyjnie: dwa tematy: \"sport\" oraz \"zdrowie\".\n",
    "* Pierwsze zdanie = 100% zdrowie\n",
    "* Drugie zdanie = 100% sport\n",
    "* Trzecie zdanie = 50% sport + 50% zdrowie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA to model probabilistyczny, który wykorzystuje dwie wartości prawdopodobieństwa: \n",
    "\n",
    "* P (słowo | tematy) \n",
    "* P (tematy | dokumenty) \n",
    "\n",
    "do stworzenia klastrów.\n",
    "\n",
    "\n",
    "Proces budowy modelu LDA zakłada więc:\n",
    "* identyfikację tematów reprezentowanych przez dokumenty korpusu,\n",
    "* oszacowanie rozkładu prawdopodobieństw wystąpienia wyrazów dla każdego z tematów (stosowany jest rozkład Dirichleta),\n",
    "* oszacowanie rozkładu prawdopodobieństw wystąpienia tematów w rozpatrywanych dokumentach (stosowany jest również rozkład Dirichleta).\n",
    "\n",
    "Wykorzystując metodę identyfikacji słów kluczowych opartą na ukrytej alokacji Dirichleta można:\n",
    "* dla każdego dokumentu obliczyć prawdopodobieństwo wystąpienia w nim każdego z tematów;\n",
    "* dla każdego tematu obliczyć prawdopodobieństwo wystąpienia słów;\n",
    "* obliczyć prawdopodobieństwa wystąpienia poszczególnych słów w dokumencie (jako iloczyny dwóch wyżej wymienionych prawdopodobieństw) \n",
    "\n",
    "# Generatywność\n",
    "\n",
    "Mając prawdopodobieństwa $p_1 = P (słowo | tematy) $, $p_2 = P (tematy | dokumenty)$ możemy wygeneraować dokument:\n",
    "\n",
    "- wybieramy (losujemy) prawdopodobieństwo przynależności dokumentu do tematu (z $p_1$) - dokuemnt zawsze należy do wielu tematów\n",
    "\n",
    "- generujemy słowa w dokumencie - najpierw losujemy do jakiego tematu należy słowo, a potem generujemy słowo z tego tematu (z $p_2$)\n",
    "\n",
    "# Uczenie\n",
    "\n",
    "1. Idziemy przez wszystkie słowa i wszystkie dokumentu i losowo przyporządkowujemy je do tematów.\n",
    "2. Iteracyjnie idzemy: bierzemy dokuemnt $d$  i słowo $w$ i przyporządkowujemy je do najlepiej pasującego tematu - maksymalizującego $P (słowo | tematy) \n",
    "* P (tematy | dokumenty) $\n",
    "3. Po przejsciu przez wszystkie dokumenty aktualizujemy przydział słów do tematów i tematów do dokumentów i wracamy do punktu 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Na początek wczytujemy korpus z dysku. Użyjemy przykład stworzonego w poprzednim notebook-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# documents = [\"Romeo and Juliet\",\n",
    "#          \"Juliet: O happy dagger\",\n",
    "#          \"Romeo died by dagger\",\n",
    "#          \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "#          \"Did you know, New-Hampshire is in New-England\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(\"tmp/deerwester.dict\")):\n",
    "    dictionary = corpora.Dictionary.load('tmp/deerwester.dict')\n",
    "    corpus = corpora.MmCorpus('tmp/deerwester.mm')\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for d in corpus:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "scipy_csc_matrix = matutils.corpus2csc(corpus)\n",
    "print(scipy_csc_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'juliet': 0, 'romeo': 1, 'dagger': 2, 'happy': 3, 'juliet:': 4, 'o': 5, 'by': 6, 'died': 7, \"'live\": 8, \"die',\": 9, 'free': 10, 'motto': 11, 'new-hampshire’s': 12, 'or': 13, 'that’s': 14, 'did': 15, 'is': 16, 'know,': 17, 'new-england': 18, 'new-hampshire': 19, 'you': 20}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budujemy model LDA\n",
    "Budujemy model LDA i transformujemy dane\n",
    "\n",
    "* **num_topics=2** oznacza ilość modelowanych tematów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.81393474), (1, 0.18606526)]\n",
      "[(0, 0.8889627), (1, 0.11103739)]\n",
      "[(0, 0.8917853), (1, 0.10821463)]\n",
      "[(0, 0.07954125), (1, 0.9204588)]\n",
      "[(0, 0.07649173), (1, 0.9235082)]\n"
     ]
    }
   ],
   "source": [
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "corpus_lda = model[corpus] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "for d in corpus_lda:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dla każdego dokumentu dostajemy prawdopodobieństwo przynależności dokumentu do danego tematu.\n",
    "\n",
    "Możemy też zobaczyć z czego składają się tematy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.105*\"dagger\" + 0.097*\"romeo\" + 0.066*\"died\" + 0.063*\"by\" + 0.062*\"o\" + 0.061*\"juliet:\" + 0.060*\"juliet\" + 0.059*\"happy\" + 0.041*\"\\'live\" + 0.040*\"that’s\"'),\n",
       " (1,\n",
       "  '0.063*\"is\" + 0.063*\"know,\" + 0.063*\"you\" + 0.063*\"did\" + 0.062*\"new-hampshire\" + 0.062*\"new-england\" + 0.056*\"new-hampshire’s\" + 0.055*\"or\" + 0.053*\"die\\',\" + 0.052*\"free\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.105*\"dagger\" + 0.097*\"romeo\" + 0.066*\"died\" + 0.063*\"by\" + 0.062*\"o\" + 0.061*\"juliet:\" + 0.060*\"juliet\" + 0.059*\"happy\" + 0.041*\"\\'live\" + 0.040*\"that’s\"'),\n",
       " (1,\n",
       "  '0.063*\"is\" + 0.063*\"know,\" + 0.063*\"you\" + 0.063*\"did\" + 0.062*\"new-hampshire\" + 0.062*\"new-england\" + 0.056*\"new-hampshire’s\" + 0.055*\"or\" + 0.053*\"die\\',\" + 0.052*\"free\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.105*\"dagger\" + 0.097*\"romeo\" + 0.066*\"died\" + 0.063*\"by\"'),\n",
       " (1, '0.063*\"is\" + 0.063*\"know,\" + 0.063*\"you\" + 0.063*\"did\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(num_topics=2, num_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Chcemy posortować słowa każdego tematu i wybrać 5 najważniejszych - co można powiedzieć o tematach?\n",
    "\n",
    "Proszę zobaczyć na funkcje typu get_topics(), get_term_topics(...): https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dagger\n",
      "romeo\n",
      "died\n",
      "by\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topics = np.argsort(model.get_topics()[0,:])[::-1] #::-1 sortowanie w odwrotnej kolejności\n",
    "for x in topics[:5]:\n",
    "    print(dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.10525187),\n",
       " (1, 0.09694249),\n",
       " (7, 0.06611413),\n",
       " (6, 0.0630177),\n",
       " (5, 0.062458374),\n",
       " (4, 0.060585562),\n",
       " (0, 0.06049739),\n",
       " (3, 0.058546674),\n",
       " (8, 0.041037835),\n",
       " (14, 0.040186286)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_terms(topicid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dagger', 0.10525187),\n",
       " ('romeo', 0.09694249),\n",
       " ('died', 0.06611413),\n",
       " ('by', 0.0630177),\n",
       " ('o', 0.062458374),\n",
       " ('juliet:', 0.060585562),\n",
       " ('juliet', 0.06049739),\n",
       " ('happy', 0.058546674),\n",
       " (\"'live\", 0.041037835),\n",
       " ('that’s', 0.040186286)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic2_terms = model.get_topic_terms(topicid=0)\n",
    "topic2_words = [\n",
    "    (dictionary.get(i), j)\n",
    "    for i,j in topic2_terms\n",
    "]\n",
    "topic2_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dagger', 0.10525187),\n",
       " ('romeo', 0.09694249),\n",
       " ('died', 0.06611413),\n",
       " ('by', 0.0630177),\n",
       " ('o', 0.062458374),\n",
       " ('juliet:', 0.060585562),\n",
       " ('juliet', 0.06049739),\n",
       " ('happy', 0.058546674),\n",
       " (\"'live\", 0.041037835),\n",
       " ('that’s', 0.040186286)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (dictionary.get(i), j)\n",
    "    for i,j in model.get_topic_terms(topicid=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Proszę posortować zdania najbardziej pasujące do danego tematu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(0, 1.0), (1, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "top_inex = 0\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(corpus[x])\n",
    "    \n",
    "#trzeba by wypisać raczej zdania niż ich reprezentacje bag-of-words, ale tu nie mam dostepu do tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(0, 1.0), (1, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "top_inex = 1\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(corpus[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo died by dagger\n",
      "Juliet: O happy dagger\n",
      "Romeo and Juliet\n",
      "'Live free or die', that’s the New-Hampshire’s motto\n",
      "Did you know, New-Hampshire is in New-England\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Romeo and Juliet\",\n",
    "         \"Juliet: O happy dagger\",\n",
    "         \"Romeo died by dagger\",\n",
    "         \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "         \"Did you know, New-Hampshire is in New-England\"]\n",
    "\n",
    "top_inex = 0\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort( numpy_corpus[top_inex,:] )[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(documents[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know, New-Hampshire is in New-England\n",
      "'Live free or die', that’s the New-Hampshire’s motto\n",
      "Romeo and Juliet\n",
      "Juliet: O happy dagger\n",
      "Romeo died by dagger\n"
     ]
    }
   ],
   "source": [
    "top_inex = 1\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(documents[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad. \n",
    "Sprawdzić do jakiego tematu pasuje nowy dokument i jakie są mu najbliższe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"died dagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8227634), (1, 0.17723662)]\n"
     ]
    }
   ],
   "source": [
    "doc_rep = dictionary.doc2bow(doc.split(' '))\n",
    "# print(doc_rep)\n",
    "doc_assignments = model[doc_rep]\n",
    "print(doc_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999208), (1, 0.9961348), (2, 0.9958265), (3, 0.29393426), (4, 0.2905712)]\n",
      "[(0, 0.9999208), (1, 0.9961348), (2, 0.9958265), (3, 0.29393426), (4, 0.2905712)]\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(corpus_lda)\n",
    "\n",
    "sims = index[doc_assignments]\n",
    "print(list(enumerate(sims)))\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wizualizacja modelu LDA:\n",
    "\n",
    "pyLDAvis\n",
    "\n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "\n",
    "* Czerwone słupki reprezentują częstotliwość słów w danym temacie (proporcjonalnie do $P (słowo \\ | \\ tematy) $), \n",
    "* Niebieskie słupki reprezentują częstotliwość tematów w całym korpusie (proporcjonalnie do $P(tematy \\ | \\ dokumenty)$. \n",
    "\n",
    "Zmień wartość $\\lambda$, aby dostosować rankingi słów: \n",
    " * małe wartości $\\lambda$ (blisko 0) podkreślają potencjalnie rzadkie, ale ekskluzywne słowa dla wybranego tematu\n",
    " * duże wartości $\\lambda$ (blisko 1) podkreślają częste, ale niekoniecznie ekskluzywne słowa dla wybranego tematu. \n",
    " \n",
    "W http://www.kennyshirley.com/LDAvis/ sugeruje się żeby ustawiać $\\lambda$ w pobliżu 0.6. Podobno pomaga to użytkownikom w interpretacji tematu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.gensim.prepare??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el111402857912195285078453980\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el111402857912195285078453980_data = {\"mdsDat\": {\"x\": [0.029657609760761258, -0.029657609760761258], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [57.536006927490234, 42.463993072509766]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8378642797470093, 0.8364725708961487, 0.8351804614067078, 0.8336867690086365, 0.8240352272987366, 0.8148918151855469, 0.7403522729873657, 0.7288451790809631, 0.6952706575393677, 0.6931278109550476, 0.6873645186424255, 0.6665988564491272, 0.6560776829719543, 0.43976858258247375, 0.41566890478134155, 0.4145794212818146, 0.39144274592399597, 0.38453224301338196, 0.5469475984573364, 0.34627851843833923, 0.4442935883998871, 1.0279654264450073, 0.6457180976867676, 0.9468100666999817, 0.6154761910438538, 0.6100133657455444, 0.5917221307754517, 0.5908609628677368, 0.5718088746070862, 0.4008049964904785, 0.39248815178871155, 0.37607139348983765, 0.37151527404785156, 0.36982107162475586, 0.34327957034111023, 0.33418211340904236, 0.2752547264099121, 0.26802632212638855, 0.2603966295719147, 0.25921520590782166, 0.2581942677497864, 0.2570941746234894], \"Term\": [\"dagger\", \"romeo\", \"died\", \"by\", \"o\", \"juliet:\", \"juliet\", \"happy\", \"is\", \"know,\", \"you\", \"did\", \"new-hampshire\", \"new-england\", \"new-hampshire\\u2019s\", \"or\", \"'live\", \"that\\u2019s\", \"die',\", \"free\", \"motto\", \"is\", \"know,\", \"you\", \"did\", \"new-hampshire\", \"new-england\", \"new-hampshire\\u2019s\", \"or\", \"die',\", \"free\", \"motto\", \"that\\u2019s\", \"'live\", \"happy\", \"juliet\", \"juliet:\", \"o\", \"by\", \"romeo\", \"died\", \"dagger\", \"dagger\", \"died\", \"romeo\", \"by\", \"o\", \"juliet:\", \"juliet\", \"happy\", \"'live\", \"that\\u2019s\", \"motto\", \"free\", \"die',\", \"or\", \"new-hampshire\\u2019s\", \"new-england\", \"new-hampshire\", \"did\", \"you\", \"know,\", \"is\"], \"Total\": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0949584245681763, 1.094666838645935, 1.094395637512207, 1.0940834283828735, 1.0920615196228027, 1.090146541595459, 1.0745344161987305, 1.072124719619751, 1.0650917291641235, 1.064643144607544, 1.0634359121322632, 1.0590870380401611, 1.056882619857788, 1.0115774869918823, 1.0065298080444336, 1.0063015222549438, 1.0014561414718628, 1.000008463859558, 1.493757724761963, 0.9919966459274292, 1.4722590446472168, 1.4722590446472168, 0.9919966459274292, 1.493757724761963, 1.000008463859558, 1.0014561414718628, 1.0063015222549438, 1.0065298080444336, 1.0115774869918823, 1.056882619857788, 1.0590870380401611, 1.0634359121322632, 1.064643144607544, 1.0650917291641235, 1.072124719619751, 1.0745344161987305, 1.090146541595459, 1.0920615196228027, 1.0940834283828735, 1.094395637512207, 1.094666838645935, 1.0949584245681763], \"loglift\": [21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.2851000130176544, 0.28369998931884766, 0.2824000120162964, 0.2809000015258789, 0.2712000012397766, 0.26170000433921814, 0.18019999563694, 0.16680000722408295, 0.12620000541210175, 0.12359999865293503, 0.11640000343322754, 0.08980000019073486, 0.07599999755620956, -0.28029999136924744, -0.33160001039505005, -0.33399999141693115, -0.38659998774528503, -0.40299999713897705, -0.45190000534057617, -0.49970000982284546, -0.6452999711036682, 0.49729999899864197, 0.42719998955726624, 0.40059998631477356, 0.3711000084877014, 0.36079999804496765, 0.3255000114440918, 0.3237999975681305, 0.28610000014305115, -0.11309999972581863, -0.13609999418258667, -0.18299999833106995, -0.19629999995231628, -0.2012999951839447, -0.2822999954223633, -0.31139999628067017, -0.5199000239372253, -0.5482000112533569, -0.5789999961853027, -0.5838000178337097, -0.5879999995231628, -0.5924999713897705], \"logprob\": [21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.7595999240875244, -2.7613000869750977, -2.7627999782562256, -2.7646000385284424, -2.7762999534606934, -2.787400007247925, -2.8833999633789062, -2.8989999294281006, -2.946199893951416, -2.9493000507354736, -2.9576001167297363, -2.988300085067749, -3.004199981689453, -3.4042000770568848, -3.460599899291992, -3.463200092315674, -3.520699977874756, -3.5385000705718994, -3.1861000061035156, -3.643199920654297, -3.3940000534057617, -2.2513999938964844, -2.716399908065796, -2.3336000442504883, -2.7643001079559326, -2.7732999324798584, -2.8036999702453613, -2.8052000999450684, -2.837899923324585, -3.1933000087738037, -3.214200019836426, -3.256999969482422, -3.2690999507904053, -3.273699998855591, -3.3482000827789307, -3.375, -3.569000005722046, -3.5957000255584717, -3.624500036239624, -3.6291000843048096, -3.632999897003174, -3.6373000144958496]}, \"token.table\": {\"Topic\": [1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1], \"Freq\": [0.9461788535118103, 0.9999915361404419, 0.6792283058166504, 0.9140070676803589, 0.938886284828186, 1.0080679655075073, 0.9392818808555603, 0.9885550141334534, 0.9132766723632812, 0.9935125708580017, 0.9937379360198975, 0.9135199785232544, 0.9403481483459473, 0.9173078536987305, 0.915699303150177, 0.9306356310844421, 0.9985460042953491, 0.9327272772789001, 0.6694526076316833, 0.6694526076316833, 0.9442094564437866, 0.9137463569641113], \"Term\": [\"'live\", \"by\", \"dagger\", \"did\", \"die',\", \"died\", \"free\", \"happy\", \"is\", \"juliet\", \"juliet:\", \"know,\", \"motto\", \"new-england\", \"new-hampshire\", \"new-hampshire\\u2019s\", \"o\", \"or\", \"romeo\", \"romeo\", \"that\\u2019s\", \"you\"]}, \"R\": 21, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el111402857912195285078453980\", ldavis_el111402857912195285078453980_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el111402857912195285078453980\", ldavis_el111402857912195285078453980_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el111402857912195285078453980\", ldavis_el111402857912195285078453980_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x    y  topics  cluster       Freq\n",
       "topic                                           \n",
       "1      0.029658  0.0       1        1  57.536007\n",
       "0     -0.029658  0.0       2        1  42.463993, topic_info=     Category      Freq             Term     Total  loglift  logprob\n",
       "term                                                                \n",
       "2     Default  1.000000           dagger  1.000000  21.0000  21.0000\n",
       "1     Default  1.000000            romeo  1.000000  20.0000  20.0000\n",
       "7     Default  0.000000             died  0.000000  19.0000  19.0000\n",
       "6     Default  1.000000               by  1.000000  18.0000  18.0000\n",
       "5     Default  1.000000                o  1.000000  17.0000  17.0000\n",
       "4     Default  1.000000          juliet:  1.000000  16.0000  16.0000\n",
       "0     Default  1.000000           juliet  1.000000  15.0000  15.0000\n",
       "3     Default  1.000000            happy  1.000000  14.0000  14.0000\n",
       "16    Default  1.000000               is  1.000000  13.0000  13.0000\n",
       "17    Default  1.000000            know,  1.000000  12.0000  12.0000\n",
       "20    Default  1.000000              you  1.000000  11.0000  11.0000\n",
       "15    Default  1.000000              did  1.000000  10.0000  10.0000\n",
       "19    Default  1.000000    new-hampshire  1.000000   9.0000   9.0000\n",
       "18    Default  1.000000      new-england  1.000000   8.0000   8.0000\n",
       "12    Default  1.000000  new-hampshire’s  1.000000   7.0000   7.0000\n",
       "13    Default  1.000000               or  1.000000   6.0000   6.0000\n",
       "8     Default  1.000000            'live  1.000000   5.0000   5.0000\n",
       "14    Default  1.000000           that’s  1.000000   4.0000   4.0000\n",
       "9     Default  1.000000            die',  1.000000   3.0000   3.0000\n",
       "10    Default  1.000000             free  1.000000   2.0000   2.0000\n",
       "11    Default  1.000000            motto  1.000000   1.0000   1.0000\n",
       "16     Topic1  0.837864               is  1.094958   0.2851  -2.7596\n",
       "17     Topic1  0.836473            know,  1.094667   0.2837  -2.7613\n",
       "20     Topic1  0.835180              you  1.094396   0.2824  -2.7628\n",
       "15     Topic1  0.833687              did  1.094083   0.2809  -2.7646\n",
       "19     Topic1  0.824035    new-hampshire  1.092062   0.2712  -2.7763\n",
       "18     Topic1  0.814892      new-england  1.090147   0.2617  -2.7874\n",
       "12     Topic1  0.740352  new-hampshire’s  1.074534   0.1802  -2.8834\n",
       "13     Topic1  0.728845               or  1.072125   0.1668  -2.8990\n",
       "9      Topic1  0.695271            die',  1.065092   0.1262  -2.9462\n",
       "...       ...       ...              ...       ...      ...      ...\n",
       "8      Topic1  0.656078            'live  1.056883   0.0760  -3.0042\n",
       "3      Topic1  0.439769            happy  1.011577  -0.2803  -3.4042\n",
       "0      Topic1  0.415669           juliet  1.006530  -0.3316  -3.4606\n",
       "4      Topic1  0.414579          juliet:  1.006302  -0.3340  -3.4632\n",
       "5      Topic1  0.391443                o  1.001456  -0.3866  -3.5207\n",
       "6      Topic1  0.384532               by  1.000008  -0.4030  -3.5385\n",
       "1      Topic1  0.546948            romeo  1.493758  -0.4519  -3.1861\n",
       "7      Topic1  0.346279             died  0.991997  -0.4997  -3.6432\n",
       "2      Topic1  0.444294           dagger  1.472259  -0.6453  -3.3940\n",
       "2      Topic2  1.027965           dagger  1.472259   0.4973  -2.2514\n",
       "7      Topic2  0.645718             died  0.991997   0.4272  -2.7164\n",
       "1      Topic2  0.946810            romeo  1.493758   0.4006  -2.3336\n",
       "6      Topic2  0.615476               by  1.000008   0.3711  -2.7643\n",
       "5      Topic2  0.610013                o  1.001456   0.3608  -2.7733\n",
       "4      Topic2  0.591722          juliet:  1.006302   0.3255  -2.8037\n",
       "0      Topic2  0.590861           juliet  1.006530   0.3238  -2.8052\n",
       "3      Topic2  0.571809            happy  1.011577   0.2861  -2.8379\n",
       "8      Topic2  0.400805            'live  1.056883  -0.1131  -3.1933\n",
       "14     Topic2  0.392488           that’s  1.059087  -0.1361  -3.2142\n",
       "11     Topic2  0.376071            motto  1.063436  -0.1830  -3.2570\n",
       "10     Topic2  0.371515             free  1.064643  -0.1963  -3.2691\n",
       "9      Topic2  0.369821            die',  1.065092  -0.2013  -3.2737\n",
       "13     Topic2  0.343280               or  1.072125  -0.2823  -3.3482\n",
       "12     Topic2  0.334182  new-hampshire’s  1.074534  -0.3114  -3.3750\n",
       "18     Topic2  0.275255      new-england  1.090147  -0.5199  -3.5690\n",
       "19     Topic2  0.268026    new-hampshire  1.092062  -0.5482  -3.5957\n",
       "15     Topic2  0.260397              did  1.094083  -0.5790  -3.6245\n",
       "20     Topic2  0.259215              you  1.094396  -0.5838  -3.6291\n",
       "17     Topic2  0.258194            know,  1.094667  -0.5880  -3.6330\n",
       "16     Topic2  0.257094               is  1.094958  -0.5925  -3.6373\n",
       "\n",
       "[63 rows x 6 columns], token_table=      Topic      Freq             Term\n",
       "term                                  \n",
       "8         1  0.946179            'live\n",
       "6         2  0.999992               by\n",
       "2         2  0.679228           dagger\n",
       "15        1  0.914007              did\n",
       "9         1  0.938886            die',\n",
       "7         2  1.008068             died\n",
       "10        1  0.939282             free\n",
       "3         2  0.988555            happy\n",
       "16        1  0.913277               is\n",
       "0         2  0.993513           juliet\n",
       "4         2  0.993738          juliet:\n",
       "17        1  0.913520            know,\n",
       "11        1  0.940348            motto\n",
       "18        1  0.917308      new-england\n",
       "19        1  0.915699    new-hampshire\n",
       "12        1  0.930636  new-hampshire’s\n",
       "5         2  0.998546                o\n",
       "13        1  0.932727               or\n",
       "1         1  0.669453            romeo\n",
       "1         2  0.669453            romeo\n",
       "14        1  0.944209           that’s\n",
       "20        1  0.913746              you, R=21, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
